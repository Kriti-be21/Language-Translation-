{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kriti-be21/Language-Translation-/blob/main/Seq2Seq_Language_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHb3nyzgYRl7",
        "outputId": "f5fc27b6-2846-4b71-8fbd-35182d6bb681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K4urXpHFTfsn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords #provides list of english stopwords\n",
        "# stop = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YlRG2HX_YI5I"
      },
      "outputs": [],
      "source": [
        "stop = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQpSM534Wum7"
      },
      "source": [
        "## **Process Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j4M0jWDITqX5"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(pd.read_csv('/content/ita.txt', sep='\\t',header = None), test_size=.10) #, nrows=100000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWZFowNtTwhO",
        "outputId": "42a3dc29-b32d-46d8-bb3f-21b3ee99850a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(146406, 2)\n",
            "(16268, 2)\n",
            "                         english                         italian\n",
            "57818        I need an umbrella.      Ho bisogno di un ombrello.\n",
            "73069       It serves you right.                    Ti sta bene.\n",
            "10027              Try the cake.               Provate la torta.\n",
            "123090  Could you wait a moment?  Potrebbe aspettare un momento?\n",
            "16293             You're boring.                   Siete noiose.\n"
          ]
        }
      ],
      "source": [
        "train.columns = ['english','italian']\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er2-vx3FTwd3",
        "outputId": "89735e7d-b973-4114-c160-1f956ef27a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-dc78ce0b7158>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train['english_no_punctuation'] = train['english_lower'].str.replace('[^\\w\\s]','')\n"
          ]
        }
      ],
      "source": [
        "train['english_lower'] = train['english'].str.lower()\n",
        "train['english_no_punctuation'] = train['english_lower'].str.replace('[^\\w\\s]','')\n",
        "#train['english_no_stopwords'] = train['english_no_punctuation'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "#train[\"english_no_stopwords\"] = train[\"english_no_stopwords\"].fillna(\"fillna\")\n",
        "#train[\"english_no_stopwords\"] = train[\"english_no_stopwords\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwkEwIxkTwbN",
        "outputId": "034f2483-f02f-41b5-e78f-49e94a589265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e0f099cc8205>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train['italian_no_punctuation'] =  '_start_' + ' ' +train['italian_lower'].str.replace('[^\\w\\s]','')+ ' ' +'_end_'\n"
          ]
        }
      ],
      "source": [
        "train['italian_lower'] = train[\"italian\"].str.lower()\n",
        "train['italian_no_punctuation'] =  '_start_' + ' ' +train['italian_lower'].str.replace('[^\\w\\s]','')+ ' ' +'_end_'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jnR3XNzVTwYi"
      },
      "outputs": [],
      "source": [
        "max_features1 = 5000\n",
        "maxlen1 = 35\n",
        "\n",
        "max_features2 = 5000\n",
        "maxlen2 = 35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Sb97nf0LTwVl"
      },
      "outputs": [],
      "source": [
        "tok1 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features1)\n",
        "tok1.fit_on_texts(list(train['english_no_punctuation'])) #fit to cleaned text\n",
        "tf_train_english =tok1.texts_to_sequences(list(train['english_no_punctuation']))\n",
        "tf_train_english =tf.keras.preprocessing.sequence.pad_sequences(tf_train_english, maxlen=maxlen1) #let's execute pad step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Wv8bejvbTwSV"
      },
      "outputs": [],
      "source": [
        "#the processing has to be done for both\n",
        "#two different tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p72qcR6mUTL4"
      },
      "outputs": [],
      "source": [
        "tok2 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features2, filters = '*')\n",
        "tok2.fit_on_texts(list(train['italian_no_punctuation'])) #fit to cleaned text\n",
        "tf_train_italian = tok2.texts_to_sequences(list(train['italian_no_punctuation']))\n",
        "tf_train_italian = tf.keras.preprocessing.sequence.pad_sequences(tf_train_italian, maxlen=maxlen2, padding ='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up4tFyreW5rS"
      },
      "source": [
        "# **Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nhyHZEdUTIg",
        "outputId": "eee48b33-550f-4226-d3ad-3e0e968e1170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of decoder input: (146406, 34)\n",
            "Shape of decoder target: (146406, 34)\n",
            "Shape of encoder input: (146406, 35)\n"
          ]
        }
      ],
      "source": [
        "vectorized_italian = tf_train_italian\n",
        "# For Decoder Input, you don't need the last word as that is only for prediction\n",
        "# when we are training using Teacher Forcing.\n",
        "decoder_input_data = vectorized_italian[:, :-1]\n",
        "\n",
        "# Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n",
        "decoder_target_data = vectorized_italian[:, 1:]\n",
        "\n",
        "print(f'Shape of decoder input: {decoder_input_data.shape}')\n",
        "print(f'Shape of decoder target: {decoder_target_data.shape}')\n",
        "\n",
        "vectorized_english = tf_train_english\n",
        "# Encoder input is simply the body of the issue text\n",
        "encoder_input_data = vectorized_english\n",
        "doc_length = encoder_input_data.shape[1]\n",
        "print(f'Shape of encoder input: {encoder_input_data.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CzDHyknGUTFz"
      },
      "outputs": [],
      "source": [
        "vocab_size_encoder = len(tok1.word_index) + 1 #remember vocab size?\n",
        "vocab_size_decoder = len(tok1.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g9oPh-7FUTCm"
      },
      "outputs": [],
      "source": [
        "#arbitrarly set latent dimension for embedding and hidden units\n",
        "latent_dim = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oex8d6e2US-4",
        "outputId": "5f051f13-eccc-4077-a2bd-d0180a52fc28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
          ]
        }
      ],
      "source": [
        "encoder_inputs = tf.keras.Input(shape=(doc_length,), name='Encoder-Input')\n",
        "\n",
        "# Word embeding for encoder (English text)\n",
        "x = tf.keras.layers.Embedding(vocab_size_encoder, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
        "\n",
        "\n",
        "#Batch normalization is used so that the distribution of the inputs\n",
        "#to a specific layer doesn't change over time\n",
        "x = tf.keras.layers.BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
        "\n",
        "\n",
        "# We do not need the `encoder_output` just the hidden state.\n",
        "_, state_h = tf.keras.layers.GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
        "\n",
        "# Encapsulate the encoder as a separate entity so we can just\n",
        "#  encode without decoding if we want to.\n",
        "encoder_model = tf.keras.Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
        "\n",
        "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
        "\n",
        "########################\n",
        "#### Decoder Model ####\n",
        "decoder_inputs = tf.keras.Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
        "\n",
        "# Word Embedding For Decoder (Italian text)\n",
        "dec_emb = tf.keras.layers.Embedding(vocab_size_decoder, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
        "#again batch normalization\n",
        "dec_bn = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
        "\n",
        "# Set up the decoder, using `decoder_state_input` as initial state.\n",
        "decoder_gru = tf.keras.layers.GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
        "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out) #the decoder \"decodes\" the encoder output.\n",
        "x = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
        "\n",
        "# Dense layer for prediction\n",
        "decoder_dense = tf.keras.layers.Dense(vocab_size_decoder, activation='softmax', name='Final-Output-Dense')\n",
        "decoder_outputs = decoder_dense(x)\n",
        "\n",
        "########################\n",
        "#### Seq2Seq Model ####\n",
        "\n",
        "seq2seq_Model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "seq2seq_Model.compile(optimizer=tf.keras.optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hrNKQlhUS79",
        "outputId": "62a42861-1506-4b83-dde0-8ea2dc6f4be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " Decoder-Input (InputLayer)  [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " Decoder-Word-Embedding (Em  (None, None, 40)             324160    ['Decoder-Input[0][0]']       \n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " Encoder-Input (InputLayer)  [(None, 35)]                 0         []                            \n",
            "                                                                                                  \n",
            " Decoder-Batchnorm-1 (Batch  (None, None, 40)             160       ['Decoder-Word-Embedding[0][0]\n",
            " Normalization)                                                     ']                            \n",
            "                                                                                                  \n",
            " Encoder-Model (Functional)  (None, 40)                   334160    ['Encoder-Input[0][0]']       \n",
            "                                                                                                  \n",
            " Decoder-GRU (GRU)           [(None, None, 40),           9840      ['Decoder-Batchnorm-1[0][0]', \n",
            "                              (None, 40)]                            'Encoder-Model[0][0]']       \n",
            "                                                                                                  \n",
            " Decoder-Batchnorm-2 (Batch  (None, None, 40)             160       ['Decoder-GRU[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " Final-Output-Dense (Dense)  (None, None, 8104)           332264    ['Decoder-Batchnorm-2[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1000744 (3.82 MB)\n",
            "Trainable params: 1000504 (3.82 MB)\n",
            "Non-trainable params: 240 (960.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#from seq2seq_utils import viz_model_architecture\n",
        "seq2seq_Model.summary()\n",
        "#viz_model_architecture(seq2seq_Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6ipoKGOXGmk"
      },
      "source": [
        "# **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckzXzWxwU3vh",
        "outputId": "b46b8509-d049-45e1-dd98-fd485f2f0ba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "108/108 [==============================] - 784s 7s/step - loss: 6.9373 - val_loss: 6.3056\n",
            "Epoch 2/6\n",
            "108/108 [==============================] - 770s 7s/step - loss: 1.7774 - val_loss: 3.0563\n",
            "Epoch 3/6\n",
            "108/108 [==============================] - 783s 7s/step - loss: 0.7861 - val_loss: 5.3677\n",
            "Epoch 4/6\n",
            "108/108 [==============================] - 764s 7s/step - loss: 0.6247 - val_loss: 7.5541\n",
            "Epoch 5/6\n",
            "108/108 [==============================] - 774s 7s/step - loss: 0.4967 - val_loss: 6.5400\n",
            "Epoch 6/6\n",
            "108/108 [==============================] - 795s 7s/step - loss: 0.4076 - val_loss: 3.5690\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1200\n",
        "epochs = 6\n",
        "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
        "          batch_size=batch_size,  epochs=epochs,  validation_split=0.12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgmrKOdxU3lh",
        "outputId": "28e00279-1970-4651-c12c-99824e4c78e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "seq2seq_Model.save('seq2seq_full_data_6_epochs.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Fb7WsP_GU3cg"
      },
      "outputs": [],
      "source": [
        "seq2seq_Model1 = seq2seq_Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dVqqvL9jU3Xl"
      },
      "outputs": [],
      "source": [
        "test_text = ['today I want to buy food']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNVcsYOxXeUB"
      },
      "source": [
        "# **Results On Holdout Set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NTXob-H5U3S7"
      },
      "outputs": [],
      "source": [
        "#max_len_title = 30\n",
        "# get the encoder's features for the decoder\n",
        "tok1.fit_on_texts(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PYUBKovAVnJy"
      },
      "outputs": [],
      "source": [
        "raw_tokenized = tok1.texts_to_sequences(test_text)\n",
        "raw_tokenized = tf.keras.preprocessing.sequence.pad_sequences(raw_tokenized, maxlen=maxlen1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z8xy7VbVnGY",
        "outputId": "a4c19c67-93d7-4538-df12-12189d8254d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 499ms/step\n"
          ]
        }
      ],
      "source": [
        "body_encoding = encoder_model.predict(raw_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Fv1sHruIVnCN"
      },
      "outputs": [],
      "source": [
        "latent_dim = seq2seq_Model.get_layer('Decoder-Word-Embedding').output_shape[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FYliqO8mVm_Y"
      },
      "outputs": [],
      "source": [
        "# Reconstruct the input into the decoder\n",
        "decoder_inputs = seq2seq_Model.get_layer('Decoder-Input').input\n",
        "dec_emb = seq2seq_Model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
        "dec_bn = seq2seq_Model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
        "# Instead of setting the intial state from the encoder and forgetting about it, during inference\n",
        "# we are not doing teacher forcing, so we will have to have a feedback loop from predictions back into\n",
        "# the GRU, thus we define this input layer for the state so we can add this capability"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru_inference_state_input = tf.keras.Input(shape=(latent_dim,), name='hidden_state_input')\n",
        "\n",
        "# we need to reuse the weights that is why we are getting this\n",
        "# If you inspect the decoder GRU that we created for training, it will take as input\n",
        "# 2 tensors -> (1) is the embedding layer output for the teacher forcing\n",
        "#                  (which will now be the last step's prediction, and will be _start_ on the first time step)\n",
        "#              (2) is the state, which we will initialize with the encoder on the first time step, but then\n",
        "#                   grab the state after the first prediction and feed that back in again."
      ],
      "metadata": {
        "id": "4W2r27WKl7Dy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_out, gru_state_out = seq2seq_Model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])"
      ],
      "metadata": {
        "id": "M8fsVZJfl_Da"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "XwxnJ5p1Vm4r"
      },
      "outputs": [],
      "source": [
        "# Reconstruct dense layers\n",
        "dec_bn2 = seq2seq_Model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
        "dense_out = seq2seq_Model.get_layer('Final-Output-Dense')(dec_bn2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "tb-HdBhZVmw7"
      },
      "outputs": [],
      "source": [
        "decoder_model = tf.keras.Model([decoder_inputs, gru_inference_state_input],\n",
        "                          [dense_out, gru_state_out])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ycQh0hV3WAgv"
      },
      "outputs": [],
      "source": [
        "# we want to save the encoder's embedding before its updated by decoder\n",
        "#   because we can use that as an embedding for other tasks.\n",
        "original_body_encoding = body_encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "z5q1pqRqWAaj"
      },
      "outputs": [],
      "source": [
        "state_value = np.array(tok2.word_index['_start_']).reshape(1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v79inr8_WAW9",
        "outputId": "408d3899-5435-410e-e92d-b4121ea7c296"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "state_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "KkgAhGEnWATZ"
      },
      "outputs": [],
      "source": [
        "decoded_sentence = []\n",
        "stop_condition = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Hcrn5n5qWN-U"
      },
      "outputs": [],
      "source": [
        "vocabulary_inv = dict((v, k) for k, v in tok2.word_index.items())\n",
        "#vocabulary_inv[0] = \"<PAD/>\"\n",
        "#vocabulary_inv[1] = \"unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIJosaAPWbWU",
        "outputId": "dc3cd6f6-58e3-463c-f600-76a6c9415c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 487ms/step\n",
            "a\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "sono\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "che\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "io\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "tom\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "non\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "è\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "stato\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "_end_\n"
          ]
        }
      ],
      "source": [
        "while not stop_condition:\n",
        "    #print(1)\n",
        "    preds, st = decoder_model.predict([state_value, body_encoding])\n",
        "    #preds = preds[preds>0]\n",
        "    # We are going to ignore indices 0 (padding) and indices 1 (unknown)\n",
        "    # Argmax will return the integer index corresponding to the\n",
        "    # prediction + 2 b/c we chopped off first two\n",
        "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
        "    #print(np.argmax(preds[:, :, 2:]))\n",
        "    # retrieve word from index prediction\n",
        "    #pred_word_str = tok.id2token[pred_idx]\n",
        "    pred_word_str = vocabulary_inv[pred_idx]\n",
        "    #print(pred_idx)\n",
        "    print(pred_word_str)\n",
        "    if pred_word_str == '_end_' or len(decoded_sentence) >= maxlen2:\n",
        "        stop_condition = True\n",
        "        break\n",
        "    decoded_sentence.append(pred_word_str)\n",
        "\n",
        "    # update the decoder for the next word\n",
        "    body_encoding = st\n",
        "    state_value = np.array(pred_idx).reshape(1, 1)\n",
        "    #print(state_value)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy4O1xp11hgSHHQ0DVOHL8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}